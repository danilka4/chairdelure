{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "619a8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.models as g\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import tokenize\n",
    "from nltk.corpus import stopwords\n",
<<<<<<< HEAD:pca.ipynb
    "import word2vec\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports the data and turns it into a list of individual messages"
=======
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.keras import layers"
>>>>>>> 840e0220dadcca1f19b538fe8d8710e337c0ab3d:notebooks/pca.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3369463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    pd.read_csv(\"data/daniel.csv\")\n",
    "    .loc[:, \"Contents\"]\n",
    "    .dropna(how=\"any\")\n",
    "    .apply(lambda x: x.lower())\n",
    "    # .apply(tokenize)\n",
    "    # .apply(list)\n",
    "    .tolist()\n",
    ")\n",
    "data = tf.data.Dataset.from_tensor_slices(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes punctuation, makes sentences lowercase, and sets up list of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "vectorize_layer.adapt(data.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'a', 'to', 'm', 'i', 'you', 'of', 'is']\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector_ds = data.batch(1024).prefetch(tf.data.AUTOTUNE).map(vectorize_layer).unbatch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258461\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   6  429   12  498   56  104    2 2991    1    8] => ['i', 'assume', 'that', 'means', 'they', 'know', 'the', 'inner', '[UNK]', 'of']\n",
      "[  3   1   1 204   1   0   0   0   0   0] => ['a', '[UNK]', '[UNK]', 'new', '[UNK]', '', '', '', '', '']\n",
      "[ 44  60 287  12  54   0   0   0   0   0] => ['more', 'or', 'less', 'that', 'yeah', '', '', '', '', '']\n",
      "[138   1  58  37  94 112 602 272   0   0] => ['class', '[UNK]', 'got', 'what', 'he', 'had', 'coming', 'smh', '', '']\n",
      "[  72    6   36  759    1    4   15  463 1905   14] => ['now', 'i', 'get', 'chat', '[UNK]', 'to', 'be', 'code', 'monke', 'for']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 258461/258461 [01:23<00:00, 3113.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (359805,)\n",
      "contexts.shape: (359805, 5)\n",
      "labels.shape: (359805, 5)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=4096,\n",
    "    seed=121)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=5)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "model = Word2Vec(vocab_size, embedding_dim)\n",
    "model.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 1.5443 - accuracy: 0.3275\n",
      "Epoch 2/20\n",
      "351/351 [==============================] - 3s 8ms/step - loss: 1.3920 - accuracy: 0.4098\n",
      "Epoch 3/20\n",
      "351/351 [==============================] - 3s 8ms/step - loss: 1.2965 - accuracy: 0.4712\n",
      "Epoch 4/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 1.2177 - accuracy: 0.5186\n",
      "Epoch 5/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 1.1481 - accuracy: 0.5567\n",
      "Epoch 6/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 1.0847 - accuracy: 0.5890\n",
      "Epoch 7/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 1.0262 - accuracy: 0.6178\n",
      "Epoch 8/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.9720 - accuracy: 0.6433\n",
      "Epoch 9/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.9217 - accuracy: 0.6664\n",
      "Epoch 10/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.8751 - accuracy: 0.6874\n",
      "Epoch 11/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.8318 - accuracy: 0.7065\n",
      "Epoch 12/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.7918 - accuracy: 0.7239\n",
      "Epoch 13/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.7548 - accuracy: 0.7398\n",
      "Epoch 14/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.7206 - accuracy: 0.7543\n",
      "Epoch 15/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.6890 - accuracy: 0.7673\n",
      "Epoch 16/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.6599 - accuracy: 0.7792\n",
      "Epoch 17/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.6329 - accuracy: 0.7898\n",
      "Epoch 18/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.6081 - accuracy: 0.7992\n",
      "Epoch 19/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.5851 - accuracy: 0.8075\n",
      "Epoch 20/20\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.5640 - accuracy: 0.8150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff6d511310>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pca.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pca.model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"pca.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
