{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nGPT2LMHeadModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFGPT2LMHeadModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m train_dataset_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/output_train.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m val_dataset_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/output_val.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 57\u001b[0m train_and_evaluate_gpt2_model(train_dataset_path, val_dataset_path)\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36mtrain_and_evaluate_gpt2_model\u001b[0;34m(train_dataset_path, test_dataset_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 9\u001b[0m model \u001b[39m=\u001b[39m GPT2LMHeadModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Load train and test datasets\u001b[39;00m\n\u001b[1;32m     12\u001b[0m train_dataset \u001b[39m=\u001b[39m TextDataset(\n\u001b[1;32m     13\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m     14\u001b[0m     file_path\u001b[39m=\u001b[39mtrain_dataset_path,\n\u001b[1;32m     15\u001b[0m     block_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/vt/chairdelure/.env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1086\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_from_config\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1086\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[0;32m~/Documents/vt/chairdelure/.env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1065\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[39m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m is_tf_available():\n\u001b[0;32m-> 1065\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[39m.\u001b[39mformat(name))\n\u001b[1;32m   1067\u001b[0m \u001b[39m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[0;31mImportError\u001b[0m: \nGPT2LMHeadModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFGPT2LMHeadModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    DataCollatorForLanguageModeling,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    TextDataset,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "\n",
    "def train_and_evaluate_gpt2_model(train_dataset_path, test_dataset_path):\n",
    "    # Load pretrained GPT-2 model and tokenizer\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Load train and test datasets\n",
    "    train_dataset = TextDataset(\n",
    "        tokenizer=tokenizer, file_path=train_dataset_path, block_size=128\n",
    "    )\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "        tokenizer=tokenizer, file_path=test_dataset_path, block_size=128\n",
    "    )\n",
    "\n",
    "    # Create data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False\n",
    "    )\n",
    "\n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./gpt2_custom\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "    )\n",
    "\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # Train and evaluate the GPT-2 model\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dataset_path = \"../data/output_train.txt\"\n",
    "    val_dataset_path = \"../data/output_val.txt\"\n",
    "    train_and_evaluate_gpt2_model(train_dataset_path, val_dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
